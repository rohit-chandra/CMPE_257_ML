{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AHVZdC-WGHLL"
   },
   "source": [
    "### What are the main motivations for reducing a dataset’s dimensionality? \n",
    "### What are the main drawbacks? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YTkXxUjkzBw9"
   },
   "source": [
    "MOTIVATION\n",
    "1. As the number of features increases the model becomes more complex and more dependent on the training data. This in turn makes the model overfitted. To avoid Overfitting DR is used\n",
    "2. Less data means less computing time, less space complexity\n",
    "3. DR removes redundant features and noise \n",
    "4. It is nearly impossible to find correlated features manually in a dataset, PCA does this efficiently and result in Principal components are independent  \n",
    "\n",
    "DRAWBACKS\n",
    "1. Can result in Data Loss\n",
    "2. DR not always increase the accuracy of the model, it varies from model to model and data to data.\n",
    "3. Independent variables becomes less interpretable \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDF-7N6pG4-O"
   },
   "source": [
    "### What are other applications of PCA (other than visualizing data)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmbbLhqdzEDl"
   },
   "source": [
    "1. To reduce the dimensionality of data to speed up training of the model and maybe better accuracy (Data Compression)\n",
    "2. Image Processing -> to reduce the number of pixels used to display an image at the same time retain the shape of the picture\n",
    "3. To remove interdependency of features from the data \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PSDKxE7HG_Z5"
   },
   "source": [
    "### What are the limitations of PCA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3eIMWQp7zFgZ"
   },
   "source": [
    "1. We need to standardize the data before implementing PCA on it else PCA wont be able to find out the Principal Components\n",
    "2. Number of Principal Components are to be chosen with care else it may result in information loss.\n",
    "3. PCA is not suitable to be applied on non-continuous data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nS_QWLDuGHLZ"
   },
   "source": [
    "### Load the MNIST dataset and split it into a training set and a test set\n",
    "### Take the first 60,000 instances for training, and the remaining 10,000 for testing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JRfK2zLRGHLZ"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "mnist = fetch_openml('mnist_784')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split( mnist.data, mnist.target, test_size=10000, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tfes2owSGHLa"
   },
   "source": [
    "### Train a Random Forest classifier on the dataset and time how long it takes, \n",
    "### then evaluate the resulting model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-637frfOGHLa"
   },
   "outputs": [],
   "source": [
    "import time \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "LRmodel = LogisticRegression()\n",
    "\n",
    "start = time.time()\n",
    "model.fit(x_train,y_train)\n",
    "end = time.time()\n",
    "\n",
    "LRstart = time.time()\n",
    "LRmodel.fit(x_train,y_train)\n",
    "LRend = time.time()\n",
    "\n",
    "y_pred = model.predict(x_test)\n",
    "print(\"Accuracy of Random Forest without PCA = \", accuracy_score(y_pred,y_test))\n",
    "print(\"Time taken by Random Forest classifier without PCA : \", end-start)\n",
    "\n",
    "print()\n",
    "\n",
    "y_pred = LRmodel.predict(x_test)\n",
    "print(\"Accuracy of Logistic Regressor without PCA = \", accuracy_score(y_pred,y_test))\n",
    "print(\"Time taken by logistic Regressor without PCA : \", LRend-LRstart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9DyypiqOGHLb"
   },
   "source": [
    "### Next, use PCA to reduce the dataset’s dimensionality, with an explained variance ratio of 95%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--8uRdpBGHLb"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler_model = StandardScaler()\n",
    "scaler_model.fit(x_train)\n",
    "\n",
    "x_train_std = scaler_model.transform(x_train)\n",
    "x_test_std = scaler_model.transform(x_test)\n",
    "\n",
    "pca_model = PCA(n_components=0.95)\n",
    "\n",
    "pca_model.fit(x_train_std)\n",
    "\n",
    "trans_train_data_df = pca_model.transform(x_train_std)\n",
    "trans_test_data_df = pca_model.transform(x_test_std)\n",
    "\n",
    "#print(trans_train_data_df.shape)\n",
    "#print(trans_test_data_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvpNwaKQGHLb"
   },
   "source": [
    "### Train a new Random Forest classifier on the reduced dataset and see how long it takes.\n",
    "### Was training much faster? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iSmomntyGHLb"
   },
   "outputs": [],
   "source": [
    "start1 = time.time()\n",
    "model.fit(trans_train_data_df,y_train)\n",
    "end1 = time.time()\n",
    "\n",
    "\n",
    "y_pred = model.predict(trans_test_data_df)\n",
    "print(\"Accuracy of Random Forest with PCA = \", accuracy_score(y_pred,y_test))\n",
    "print(\"Time taken by Random Forest classifier with PCA : \", end1-start1)\n",
    "\n",
    "print() \n",
    "\n",
    "LRstart1 = time.time()\n",
    "LRmodel.fit(trans_train_data_df,y_train)\n",
    "LRend1 = time.time()\n",
    "\n",
    "y_pred = LRmodel.predict(trans_test_data_df)\n",
    "print(\"Accuracy of Logistic Regressor with PCA = \", accuracy_score(y_pred,y_test))\n",
    "print(\"Time taken by Logistic Regressor with PCA : \", LRend1-LRstart1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F1u9d4FpGHLc"
   },
   "source": [
    "### Next evaluate the classifier on the test set: how does it compare to the previous classifier?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIDT7FId5sk-"
   },
   "source": [
    "OBSERVATIONS\n",
    "\n",
    "The time taken by logistic regressor model is more when the data is feeded as it is. While after performing PCA the model takes much less time for training and also there is slight increase in the accuracy \n",
    "\n",
    "While for Random forest model time taken to train the model after performing PCA is much higher as compared to the raw data feeded to the model. Also the accuracy is reduced when reduced data is used to train the model.\n",
    "\n",
    "CONCLUSION\n",
    "\n",
    "PCA does not always ensure the best result. It depends on the model being used and the type of data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW 10.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
